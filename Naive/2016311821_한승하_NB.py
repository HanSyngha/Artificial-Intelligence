# -*- coding: utf-8 -*-
"""2016311821_한승하_NB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-xXCPU4dbo4b4EGZytwMjVN7_dmJBxqE
"""

import json
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import DictVectorizer
from sklearn.naive_bayes import MultinomialNB
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
fd = open('./2016311821_한승하_NB.txt','w') 

#opening files
with open("./bbc_articles_train.json",'r') as train_file:
  train_data = json.load(train_file)
  train_business = train_data['business']
  train_tech = train_data['tech']
  train_politics = train_data['politics']

with open("./bbc_articles_test.json",'r') as test_file:
  test_data = json.load(test_file)
  test_business = test_data['business']
  test_tech = test_data['tech']
  test_politics = test_data['politics']

#Make it to one list
train_corpus = list()
train_labels = list()
test_corpus = list()
test_labels = list()
for sentence in train_business:
  train_corpus.append(sentence)
  train_labels.append("business")
for sentence in train_tech:
  train_corpus.append(sentence)
  train_labels.append("tech")
for sentence in train_politics:
  train_corpus.append(sentence)
  train_labels.append("politics")

for sentence in test_business:
  test_corpus.append(sentence)
  test_labels.append("business")
for sentence in test_tech:
  test_corpus.append(sentence)
  test_labels.append("tech")
for sentence in test_politics:
  test_corpus.append(sentence)
  test_labels.append("politics")

#making TF List
POS_tags = ['NN','NNS','NNP','NNPS','VB','VBD','VBG','VBN','VBP','VBZ']

Train_Frequency = list()
Test_Frequency = list()
BOW = DictVectorizer(sparse=False)


for sentence in train_corpus:
  POS_corpus_train = list()
  pos_token = nltk.pos_tag(word_tokenize(sentence))
  POS_corpus_train.append(' '.join(t[0].lower() for t in pos_token if t[1] in POS_tags))
  #TF calculation
  vect = CountVectorizer()
  vect.fit_transform(POS_corpus_train)
  Train_Frequency.append(vect.vocabulary_)

for sentence in test_corpus:
  POS_corpus_test = list()
  pos_token = nltk.pos_tag(word_tokenize(sentence))
  POS_corpus_test.append(' '.join(t[0].lower() for t in pos_token if t[1] in POS_tags))
  #TF calculation
  vect = CountVectorizer()
  vect.fit_transform(POS_corpus_test)
  Test_Frequency.append(vect.vocabulary_)

#TF Features
Train_feature = BOW.fit_transform(Train_Frequency)
Test_feature = BOW.transform(Test_Frequency)

#Pridiction
classifier = MultinomialNB()
classifier.fit(Train_feature,train_labels)
prediction = classifier.predict(Test_feature)

#confusion matrix
print('Confusion matrix:',file=fd)
conf = confusion_matrix(test_labels, prediction)
for tup in conf:
  for num in tup:
    print(num,end="\t",file=fd)
  print("\n",file=fd)

# Accuracy
acc = "Accuracy : %.4f"%(accuracy_score(test_labels, prediction) * 100)
print(acc+"%",end="\n\n",file=fd)

# Precision
prec1 = "Macro averaging precision : %.4f"%(precision_score(test_labels, prediction, average='macro') * 100)
print(prec1+"%",file=fd)
prec2 = "Micro averaging precision : %.4f"%(precision_score(test_labels, prediction, average='micro') * 100)
print(prec2+"%",end="\n\n",file=fd)

# Recall
recal1 = "Macro averaging recall : %.4f"%(recall_score(test_labels, prediction, average='macro') * 100)
print(recal1 + "%",file=fd)
recal2 = "Micro averaging recall : %.4f"%(recall_score(test_labels, prediction, average='micro') * 100)
print(recal2 + "%",end="\n\n",file=fd)

# F1-score
F11 = "Macro averaging f1-score : %.4f"%(f1_score(test_labels, prediction, average='macro') * 100)
print(F11+"%",file=fd)
F12 = "Micro averaging f1-score : %.4f"%(f1_score(test_labels, prediction, average='micro') * 100)
print(F12+"%",file=fd)

fd.close()



