# -*- coding: utf-8 -*-
"""2016311821_한승하.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCxyS85xQf7DwTG2F08ZNVuWNMq7fsQ3
"""

import json

import nltk
from nltk.tokenize import word_tokenize

def tokenizer(doc):
    return ["/".join(string) for string in tagged_tokens]


nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


with open('./bbc_articles.json') as json_file:
    data = json.load(json_file)
    json_string = data["business"]
    json_tech = data["tech"]
    json_politics = data["politics"]
f = open("2016311821_한승하.txt",'w')
for words in json_string:
  tokens = word_tokenize(words)
  tagged_tokens = nltk.pos_tag(tokens)
  print_d = tokenizer(words)
  f.write(words)
  f.write("\n")
  f.write ("+".join(print_d))
  f.write("\n\n")

for words in json_tech:
  tokens = word_tokenize(words)
  tagged_tokens = nltk.pos_tag(tokens)
  print_d = tokenizer(words)
  f.write(words)
  f.write("\n")
  f.write ("+".join(print_d))
  f.write("\n\n")

for words in json_politics:
  tokens = word_tokenize(words)
  tagged_tokens = nltk.pos_tag(tokens)
  print_d = tokenizer(words)
  f.write(words)
  f.write("\n")
  f.write ("+".join(print_d))
  f.write("\n\n")
f.close()

